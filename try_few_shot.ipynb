{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0. Setup: installs & imports\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q huggingface_hub torch torchvision matplotlib\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Download Geneva dataset\n",
    "# ============================================================\n",
    "\n",
    "dataset_root = snapshot_download(repo_id=\"raphaelattias/overfitteam-geneva-satellite-images\", repo_type=\"dataset\")\n",
    "print(\"Dataset root:\", dataset_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. Base dataset: load images & masks with transforms\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "IMAGE_SIZE = 256  # resize tiles to this\n",
    "\n",
    "img_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mask_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n",
    "        transforms.ToTensor(),  # gives float [0,1] for grayscale\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class GenevaRooftopDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Regular (image, mask) dataset for one split and category.\n",
    "    Mask is binary: 1 = rooftop suitable for PV, 0 = background.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, split=\"train\", category=\"all\"):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.category = category\n",
    "\n",
    "        self.image_dir = os.path.join(root, split, \"images\", category)\n",
    "        self.label_dir = os.path.join(root, split, \"labels\", category)\n",
    "\n",
    "        self.filenames = sorted([f for f in os.listdir(self.image_dir) if f.endswith(\".png\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.filenames[idx]\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, fname)\n",
    "        label_name = fname.replace(\".png\", \"_label.png\")\n",
    "        mask_path = os.path.join(self.label_dir, label_name)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        # Apply transforms\n",
    "        img = img_transform(img)\n",
    "        mask = mask_transform(mask)  # [1, H, W], float\n",
    "\n",
    "        # Binarise: any non-zero pixel -> 1\n",
    "        mask = (mask > 0.5).float()\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "# Instantiate datasets (we focus on 'all' category)\n",
    "train_base = GenevaRooftopDataset(dataset_root, split=\"train\", category=\"all\")\n",
    "val_base = GenevaRooftopDataset(dataset_root, split=\"val\", category=\"all\")\n",
    "test_base = GenevaRooftopDataset(dataset_root, split=\"test\", category=\"all\")\n",
    "\n",
    "print(f\"Train samples: {len(train_base)}, Val: {len(val_base)}, Test: {len(test_base)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. Quick visualisation: image + mask for sanity check\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def show_sample(dataset, idx=None):\n",
    "    if idx is None:\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "    img, mask = dataset[idx]  # img [3,H,W], mask [1,H,W]\n",
    "\n",
    "    # Undo normalisation for plotting\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "    # mask: [1, H, W] -> [H, W]\n",
    "    mask_np = mask.squeeze(0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_np)\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask_np, cmap=\"gray\")\n",
    "    plt.title(\"Mask (rooftop)\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_sample(train_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. Few-shot / episodic dataset: support + query\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "class EpisodeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Yields (support_image, support_mask, query_image, query_mask).\n",
    "    We sample two different tiles from the base training set for each episode.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_dataset, episodes_per_epoch=500):\n",
    "        self.base = base_dataset\n",
    "        self.episodes_per_epoch = episodes_per_epoch\n",
    "        self.n = len(base_dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.episodes_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, j = random.sample(range(self.n), 2)\n",
    "        img_s, mask_s = self.base[i]\n",
    "        img_q, mask_q = self.base[j]\n",
    "        return img_s, mask_s, img_q, mask_q\n",
    "\n",
    "\n",
    "episodes_per_epoch = 500  # adjust for speed vs quality\n",
    "episode_dataset = EpisodeDataset(train_base, episodes_per_epoch=episodes_per_epoch)\n",
    "episode_loader = DataLoader(episode_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. Encoder backbone for few-shot segmentation\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional encoder using ResNet18 up to layer3 (downsampling by 8),\n",
    "    followed by a 1x1 conv to get a compact embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_channels=256, pretrained=True):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet18(pretrained=pretrained)\n",
    "        self.features = nn.Sequential(\n",
    "            backbone.conv1,\n",
    "            backbone.bn1,\n",
    "            backbone.relu,\n",
    "            backbone.maxpool,\n",
    "            backbone.layer1,\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "        )\n",
    "        # resnet18.layer3 output has 256 channels\n",
    "        self.proj = nn.Conv2d(256, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)  # [B, 256, H', W']\n",
    "        f = self.proj(f)  # [B, C,   H', W']\n",
    "        return f\n",
    "\n",
    "\n",
    "encoder = Encoder(out_channels=256, pretrained=True).to(device)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Prototype computation & query classification\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def compute_prototypes(feat_support, mask_support):\n",
    "    \"\"\"\n",
    "    Compute background/foreground prototypes from multiple support images.\n",
    "\n",
    "    feat_support: [K, C, H', W']   (K support images)\n",
    "    mask_support: [K, 1, H,  W]    (binary masks)\n",
    "    Returns: prototypes [2, C] (0=background, 1=foreground)\n",
    "    \"\"\"\n",
    "    # Downsample mask to feature resolution\n",
    "    mask_small = F.interpolate(mask_support, size=feat_support.shape[2:], mode=\"nearest\")\n",
    "    mask_fg = (mask_small > 0.5).float()  # [K,1,H',W']\n",
    "    mask_bg = 1.0 - mask_fg  # [K,1,H',W']\n",
    "\n",
    "    K, C, Hf, Wf = feat_support.shape\n",
    "\n",
    "    # Flatten across batch and spatial dims: [K,C,H',W'] -> [C, K*H'*W']\n",
    "    fs = feat_support.permute(1, 0, 2, 3).contiguous().view(C, -1)  # [C, K*H'*W']\n",
    "    fg_w = mask_fg.view(1, -1)  # [1, K*H'*W']\n",
    "    bg_w = mask_bg.view(1, -1)\n",
    "\n",
    "    eps = 1e-6\n",
    "\n",
    "    # Weighted average for foreground\n",
    "    fg_proto = (fs * fg_w).sum(dim=1) / (fg_w.sum(dim=1) + eps)  # [C]\n",
    "    # Weighted average for background\n",
    "    bg_proto = (fs * bg_w).sum(dim=1) / (bg_w.sum(dim=1) + eps)  # [C]\n",
    "\n",
    "    prototypes = torch.stack([bg_proto, fg_proto], dim=0)  # [2,C]\n",
    "    return prototypes\n",
    "\n",
    "\n",
    "def classify_query(feat_query, prototypes):\n",
    "    \"\"\"\n",
    "    Classify query pixels by distance to prototypes.\n",
    "\n",
    "    feat_query: [1, C, H', W']\n",
    "    prototypes: [2, C]\n",
    "    Returns: logits [1, 2, H', W']\n",
    "    \"\"\"\n",
    "    B, C, Hq, Wq = feat_query.shape\n",
    "\n",
    "    # [1,C,H',W'] -> [H'*W', C]\n",
    "    fq = feat_query.view(C, -1).t()  # [H'*W', C]\n",
    "\n",
    "    # [2, C]\n",
    "    protos = prototypes  # [2,C]\n",
    "\n",
    "    # Compute squared Euclidean distance from each pixel to each prototype\n",
    "    # torch.cdist expects [B, N, D], so add batch dim\n",
    "    # fq_batch: [1, H'*W', C], protos_batch: [1, 2, C]\n",
    "    dists = torch.cdist(fq.unsqueeze(0), protos.unsqueeze(0))  # [1, H'*W', 2]\n",
    "    dists = dists.squeeze(0)  # [H'*W', 2]\n",
    "    dists = dists**2\n",
    "\n",
    "    # Convert distances to similarity logits: negative distance\n",
    "    logits_flat = -dists  # [H'*W', 2]\n",
    "    logits = logits_flat.t().view(1, 2, Hq, Wq)  # [1,2,H',W']\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. IoU metric for evaluation\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def iou_from_logits(logits, target_mask, eps=1e-6):\n",
    "    \"\"\"\n",
    "    logits: [1,2,H,W] (class 0=background, 1=foreground)\n",
    "    target_mask: [1,1,H,W], binary 0/1\n",
    "    \"\"\"\n",
    "    # predicted class (0 or 1)\n",
    "    pred = logits.argmax(dim=1, keepdim=True).float()  # [1,1,H,W]\n",
    "    target = (target_mask > 0.5).float()\n",
    "\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "\n",
    "    iou = (intersection + eps) / (union + eps)\n",
    "    return iou.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. Episodic meta-training loop\n",
    "# ============================================================\n",
    "\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "def meta_train(num_epochs=5):\n",
    "    encoder.train()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for img_s, mask_s, img_q, mask_q in episode_loader:\n",
    "            img_s = img_s.to(device)\n",
    "            mask_s = mask_s.to(device)\n",
    "            img_q = img_q.to(device)\n",
    "            mask_q = mask_q.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 1) Encode support & query\n",
    "            feat_s = encoder(img_s)  # [1,C,H',W']\n",
    "            feat_q = encoder(img_q)  # [1,C,H',W']\n",
    "\n",
    "            # 2) Prototypes from support\n",
    "            prototypes = compute_prototypes(feat_s, mask_s)  # [2,C]\n",
    "\n",
    "            # 3) Classify query pixels\n",
    "            logits_q_small = classify_query(feat_q, prototypes)  # [1,2,H',W']\n",
    "\n",
    "            # 4) Upsample logits to original mask size\n",
    "            logits_q = F.interpolate(\n",
    "                logits_q_small, size=mask_q.shape[2:], mode=\"bilinear\", align_corners=False\n",
    "            )  # [1,2,H,W]\n",
    "\n",
    "            # 5) Cross-entropy loss; target: 0/1\n",
    "            target_q = mask_q.long().squeeze(1)  # [1,H,W] with values {0,1}\n",
    "            loss = F.cross_entropy(logits_q, target_q)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(episode_loader)\n",
    "        print(f\"Epoch {epoch}/{num_epochs} | avg episode loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Run meta-training\n",
    "meta_train(num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. Few-shot inference on test images\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def k_shot_predict(encoder, support_imgs, support_masks, query_img):\n",
    "    \"\"\"\n",
    "    K-shot segmentation for a query image given K support images+masks.\n",
    "\n",
    "    support_imgs:  [K, 3, H, W]\n",
    "    support_masks: [K, 1, H, W]\n",
    "    query_img:     [3, H, W]\n",
    "    Returns: logits [1, 2, H, W]\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        support_imgs = support_imgs.to(device)  # [K,3,H,W]\n",
    "        support_masks = support_masks.to(device)  # [K,1,H,W]\n",
    "        query_img = query_img.to(device).unsqueeze(0)  # [1,3,H,W]\n",
    "\n",
    "        feat_s = encoder(support_imgs)  # [K,C,H',W']\n",
    "        feat_q = encoder(query_img)  # [1,C,H',W']\n",
    "\n",
    "        prototypes = compute_prototypes(feat_s, support_masks)  # [2,C]\n",
    "\n",
    "        logits_small = classify_query(feat_q, prototypes)  # [1,2,H',W']\n",
    "        logits = F.interpolate(\n",
    "            logits_small,\n",
    "            size=(query_img.shape[2], query_img.shape[3]),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )  # [1,2,H,W]\n",
    "\n",
    "    return logits.cpu()\n",
    "\n",
    "\n",
    "def one_shot_predict(encoder, support_img, support_mask, query_img):\n",
    "    \"\"\"\n",
    "    1-shot helper that wraps single support into K=1 form.\n",
    "    \"\"\"\n",
    "    support_imgs = support_img.unsqueeze(0)  # [1,3,H,W]\n",
    "    support_masks = support_mask.unsqueeze(0)  # [1,1,H,W]\n",
    "    return k_shot_predict(encoder, support_imgs, support_masks, query_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9a. K-shot inference on test images\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def evaluate_kshot_iou(encoder, train_dataset, test_dataset, K=5, num_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate K-shot IoU on 'num_samples' random test images.\n",
    "    For each test image, randomly sample K *distinct* support images from the train set.\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    # If num_samples is None, evaluate on all test samples\n",
    "    if num_samples is None:\n",
    "        num_samples = len(test_dataset)\n",
    "\n",
    "    ious = []\n",
    "    for _ in range(num_samples):\n",
    "        # pick random test index\n",
    "        ti = rng.integers(0, len(test_dataset))\n",
    "        img_q, mask_q = test_dataset[ti]\n",
    "\n",
    "        # pick K distinct support indices\n",
    "        support_indices = rng.choice(len(train_dataset), size=K, replace=False)\n",
    "        support_imgs = []\n",
    "        support_masks = []\n",
    "        for si in support_indices:\n",
    "            img_s, mask_s = train_dataset[si]\n",
    "            support_imgs.append(img_s)\n",
    "            support_masks.append(mask_s)\n",
    "        support_imgs = torch.stack(support_imgs, dim=0)  # [K,3,H,W]\n",
    "        support_masks = torch.stack(support_masks, dim=0)  # [K,1,H,W]\n",
    "\n",
    "        # run K-shot prediction\n",
    "        logits = k_shot_predict(encoder, support_imgs, support_masks, img_q)  # [1,2,H,W]\n",
    "\n",
    "        iou = iou_from_logits(logits, mask_q.unsqueeze(0))\n",
    "        ious.append(iou)\n",
    "\n",
    "    ious = np.array(ious)\n",
    "    print(f\"{K}-shot mean IoU over {num_samples} test samples: {ious.mean():.3f} Â± {ious.std():.3f}\")\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9b. Visualise a 1-shot episode (support + query + prediction)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def tensor_to_rgb(img_tensor):\n",
    "    \"\"\"Undo normalisation and convert [3,H,W] tensor to [H,W,3] RGB numpy.\"\"\"\n",
    "    img_np = img_tensor.detach().cpu().permute(1, 2, 0).numpy()\n",
    "    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "    return img_np\n",
    "\n",
    "\n",
    "def visualise_few_shot_example(encoder, train_dataset, test_dataset):\n",
    "    encoder.eval()\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # pick support from train, query from test\n",
    "    si = rng.integers(0, len(train_dataset))\n",
    "    ti = rng.integers(0, len(test_dataset))\n",
    "\n",
    "    img_s, mask_s = train_dataset[si]\n",
    "    img_q, mask_q = test_dataset[ti]\n",
    "\n",
    "    logits = one_shot_predict(encoder, img_s, mask_s, img_q)  # [1,2,H,W]\n",
    "    pred_mask = logits.argmax(dim=1, keepdim=True).float().squeeze(0).squeeze(0).numpy()\n",
    "\n",
    "    # convert to numpy for plotting\n",
    "    img_s_np = tensor_to_rgb(img_s)\n",
    "    img_q_np = tensor_to_rgb(img_q)\n",
    "    mask_s_np = mask_s.squeeze(0).numpy()\n",
    "    mask_q_np = mask_q.squeeze(0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(img_s_np)\n",
    "    plt.title(\"Support image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.imshow(mask_s_np, cmap=\"gray\")\n",
    "    plt.title(\"Support mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.imshow(img_q_np)\n",
    "    plt.title(\"Query image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.imshow(mask_q_np, cmap=\"gray\")\n",
    "    plt.title(\"Query GT mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.imshow(pred_mask, cmap=\"gray\")\n",
    "    plt.title(\"Predicted mask (1-shot)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualise_few_shot_example(encoder, train_base, test_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9d. K-shot inference on test images\n",
    "# ============================================================\n",
    "\n",
    "# 1-shot using the same function\n",
    "ious_1shot = evaluate_kshot_iou(encoder, train_base, test_base, K=1, num_samples=None)\n",
    "\n",
    "# K-shot\n",
    "ious_5shot = evaluate_kshot_iou(encoder, train_base, test_base, K=5, num_samples=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9e. Visualise a K-shot episode (support + query + prediction)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def visualise_kshot_example(encoder, train_dataset, test_dataset, K=5):\n",
    "    encoder.eval()\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # pick query from test\n",
    "    ti = rng.integers(0, len(test_dataset))\n",
    "    img_q, mask_q = test_dataset[ti]\n",
    "\n",
    "    # pick K supports from train\n",
    "    support_indices = rng.choice(len(train_dataset), size=K, replace=False)\n",
    "    support_imgs, support_masks = [], []\n",
    "    for si in support_indices:\n",
    "        img_s, mask_s = train_dataset[si]\n",
    "        support_imgs.append(img_s)\n",
    "        support_masks.append(mask_s)\n",
    "    support_imgs = torch.stack(support_imgs, dim=0)  # [K,3,H,W]\n",
    "    support_masks = torch.stack(support_masks, dim=0)  # [K,1,H,W]\n",
    "\n",
    "    # prediction\n",
    "    logits = k_shot_predict(encoder, support_imgs, support_masks, img_q)\n",
    "    pred_mask = logits.argmax(dim=1, keepdim=True).float().squeeze().numpy()\n",
    "\n",
    "    img_q_np = tensor_to_rgb(img_q)\n",
    "    mask_q_np = mask_q.squeeze(0).numpy()\n",
    "\n",
    "    # Plot\n",
    "    cols = max(K, 3)\n",
    "    plt.figure(figsize=(4 * cols, 8))\n",
    "\n",
    "    # first row: support images\n",
    "    for i in range(K):\n",
    "        plt.subplot(2, cols, i + 1)\n",
    "        plt.imshow(tensor_to_rgb(support_imgs[i]))\n",
    "        plt.title(f\"Support {i+1} image\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    # second row: support masks\n",
    "    for i in range(K):\n",
    "        plt.subplot(2, cols, cols + i + 1)\n",
    "        plt.imshow(support_masks[i].squeeze(0).numpy(), cmap=\"gray\")\n",
    "        plt.title(f\"Support {i+1} mask\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    # query image & masks to the right (replace last columns if needed)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img_q_np)\n",
    "    plt.title(\"Query image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask_q_np, cmap=\"gray\")\n",
    "    plt.title(\"Query GT mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred_mask, cmap=\"gray\")\n",
    "    plt.title(f\"Predicted mask ({K}-shot)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example 5-shot visualisation\n",
    "visualise_kshot_example(encoder, train_base, test_base, K=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
