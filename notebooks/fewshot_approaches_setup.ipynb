{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Learning for Rooftop Segmentation: Approach Setup\n",
    "\n",
    "#### ATTENTION: This notebook is AI generated, and should be used as a checklist / orientative document. Information have been checked, and should be acceptably accurate.\n",
    "\n",
    "**Goal**: Explore three few-shot learning approaches for semantic segmentation of rooftops across different geographic regions.\n",
    "\n",
    "**Scenario**: We have a model trained on one area of Geneva (grid 1301_11) and want to deploy it to other areas (grids 1301_13 and 1301_31) with only K labeled examples from each target area.\n",
    "\n",
    "**Real-world motivation**: A city government trained a rooftop segmentation model for solar panel assessment. They now want to deploy it to neighboring cities/districts, but labeling is expensive. Can we achieve good performance with only 5-10 labeled examples from the new area?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dataset and Geographic Split Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giocopp/miniconda3/envs/DL-Tutorial/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/giocopp/miniconda3/envs/DL-Tutorial/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `snapshot_download`. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n",
      "Fetching ... files: 2111it [00:00, 8405.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: /Users/giocopp/Desktop/Uni/Hertie School/5th Semester/DL/DL-FinalProject/DL-Tutorial-giorgio-exploration/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# Download dataset\n",
    "data_dir = Path(\"data\")\n",
    "dataset_path = snapshot_download(\n",
    "    repo_id=\"raphaelattias/overfitteam-geneva-satellite-images\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=str(data_dir),\n",
    "    local_dir_use_symlinks=True,\n",
    ")\n",
    "\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Geographic Split Strategy\n",
    "\n",
    "The Geneva dataset contains images from three geographic grids:\n",
    "- **Grid 1301_11**: ~70% of data (295 train images)\n",
    "- **Grid 1301_13**: ~18% of data (76 train images) \n",
    "- **Grid 1301_31**: ~12% of data (49 train images) \n",
    "\n",
    "**Our Split**:\n",
    "```\n",
    "Source Domain (Base Training):  Grid 1301_11 only\n",
    "Target Domains (Few-Shot Test): Grid 1301_13 and Grid 1301_31\n",
    "```\n",
    "\n",
    "This simulates **geographic domain shift**: training on one part of the city and deploying to other parts with different building densities, roof types, and urban layouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAINING DATA BY GRID ===\n",
      "Grid 1301_11: 295 images\n",
      "Grid 1301_13: 76 images\n",
      "Grid 1301_31: 49 images\n",
      "\n",
      "=== OUR SPLIT STRATEGY ===\n",
      "Source domain (base training):  Grid 1301_11 (295 images)\n",
      "Target domain 1 (few-shot):     Grid 1301_13 (76 images)\n",
      "Target domain 2 (few-shot):     Grid 1301_31 (49 images)\n"
     ]
    }
   ],
   "source": [
    "def parse_grid_from_filename(filepath):\n",
    "    \"\"\"\n",
    "    Extract grid ID from filename.\n",
    "    Example: DOP25_LV03_1301_11_2015_1_15_497500.0_119062.5.png -> '1301_11'\n",
    "    \"\"\"\n",
    "    parts = Path(filepath).stem.split(\"_\")\n",
    "    sheet, subgrid = parts[2], parts[3]\n",
    "    return f\"{sheet}_{subgrid}\"\n",
    "\n",
    "\n",
    "def create_geographic_split(dataset_path, split=\"train\", category=\"all\"):\n",
    "    \"\"\"\n",
    "    Organize images by geographic grid.\n",
    "\n",
    "    Returns:\n",
    "        dict: {grid_id: [(image_path, label_path), ...]}\n",
    "\n",
    "    \"\"\"\n",
    "    img_dir = Path(dataset_path) / split / \"images\" / category\n",
    "    label_dir = Path(dataset_path) / split / \"labels\" / category\n",
    "\n",
    "    grid_data = defaultdict(list)\n",
    "\n",
    "    for img_path in sorted(img_dir.glob(\"*.png\")):\n",
    "        grid_id = parse_grid_from_filename(img_path)\n",
    "        label_path = label_dir / img_path.name.replace(\".png\", \"_label.png\")\n",
    "\n",
    "        if label_path.exists():\n",
    "            grid_data[grid_id].append((str(img_path), str(label_path)))\n",
    "\n",
    "    return grid_data\n",
    "\n",
    "\n",
    "# Create splits\n",
    "train_by_grid = create_geographic_split(dataset_path, split=\"train\", category=\"all\")\n",
    "val_by_grid = create_geographic_split(dataset_path, split=\"val\", category=\"all\")\n",
    "test_by_grid = create_geographic_split(dataset_path, split=\"test\", category=\"all\")\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n=== TRAINING DATA BY GRID ===\")\n",
    "for grid_id in sorted(train_by_grid.keys()):\n",
    "    print(f\"Grid {grid_id}: {len(train_by_grid[grid_id])} images\")\n",
    "\n",
    "print(\"\\n=== OUR SPLIT STRATEGY ===\")\n",
    "print(f\"Source domain (base training):  Grid 1301_11 ({len(train_by_grid['1301_11'])} images)\")\n",
    "print(f\"Target domain 1 (few-shot):     Grid 1301_13 ({len(train_by_grid['1301_13'])} images)\")\n",
    "print(f\"Target domain 2 (few-shot):     Grid 1301_31 ({len(train_by_grid['1301_31'])} images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Approach 1: Fine-Tuning (Transfer Learning)\n",
    "\n",
    "### Concept\n",
    "Traditional transfer learning approach: pre-train a model on the source domain, then fine-tune on K labeled examples from the target domain.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "**Phase 1: Base Model Training**\n",
    "```\n",
    "Training data: All images from Grid 1301_11 (~295 images)\n",
    "Architecture:  U-Net or DeepLabV3 with ResNet encoder\n",
    "Task:          Binary segmentation (rooftop vs. background)\n",
    "Optimization:  Standard supervised learning with cross-entropy + Dice loss\n",
    "```\n",
    "\n",
    "**Phase 2: Few-Shot Adaptation (Fine-Tuning)**\n",
    "```\n",
    "For each target grid (1301_13, 1301_31):\n",
    "    1. Randomly select K images as support set (K = 1, 3, 5, 10, 20)\n",
    "    2. Fine-tune the pre-trained model on these K examples\n",
    "    3. Evaluate on remaining images from that grid\n",
    "```\n",
    "\n",
    "**Fine-Tuning Strategies**:\n",
    "- **Full fine-tuning**: Update all model parameters\n",
    "- **Partial fine-tuning**: Freeze encoder, only update decoder\n",
    "- **Learning rate**: Use small LR (1e-5 to 1e-4) to avoid catastrophic forgetting\n",
    "- **Early stopping**: Monitor validation loss to prevent overfitting on K examples\n",
    "\n",
    "### Advantages\n",
    "- ✅ Simple to implement and understand\n",
    "- ✅ Well-established baseline in transfer learning\n",
    "- ✅ Works with any architecture\n",
    "- ✅ Fast adaptation (just a few gradient updates)\n",
    "\n",
    "### Limitations\n",
    "- ❌ Prone to overfitting when K is very small (K=1,3)\n",
    "- ❌ Doesn't explicitly learn \"how to learn from few examples\"\n",
    "- ❌ Requires careful hyperparameter tuning (LR, epochs, which layers to freeze)\n",
    "- ❌ May suffer from catastrophic forgetting\n",
    "\n",
    "### Expected Performance\n",
    "```\n",
    "Baseline (zero-shot, no adaptation):  ~XX% IoU on target grids\n",
    "Fine-tuning with K=1:                 ~XX% IoU (minimal improvement)\n",
    "Fine-tuning with K=5:                 ~XX% IoU (moderate improvement)\n",
    "Fine-tuning with K=20:                ~XX% IoU (good improvement)\n",
    "```\n",
    "\n",
    "### Implementation Outline (Not Implemented Yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning approach pseudocode\n",
    "\"\"\"\n",
    "# Phase 1: Train base model\n",
    "base_model = UNet(encoder='resnet34', classes=2)\n",
    "train_dataset = Grid_1301_11_Dataset(train_by_grid['1301_11'])\n",
    "trainer = train(base_model, train_dataset, epochs=50)\n",
    "save_checkpoint(base_model, 'base_model.pth')\n",
    "\n",
    "# Phase 2: Few-shot fine-tuning\n",
    "for target_grid in ['1301_13', '1301_31']:\n",
    "    for K in [1, 3, 5, 10, 20]:\n",
    "        # Sample K support examples\n",
    "        support_set = random.sample(train_by_grid[target_grid], K)\n",
    "        query_set = [x for x in train_by_grid[target_grid] if x not in support_set]\n",
    "        \n",
    "        # Load pre-trained base model\n",
    "        model = load_checkpoint('base_model.pth')\n",
    "        \n",
    "        # Fine-tune on K examples\n",
    "        optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "        for epoch in range(10):  # Small number of epochs\n",
    "            for img, mask in support_set:\n",
    "                loss = compute_loss(model(img), mask)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluate on query set\n",
    "        metrics = evaluate(model, query_set)\n",
    "        print(f\"Grid {target_grid}, K={K}: IoU = {metrics['iou']:.3f}\")\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Approach 2: Meta-Learning with Prototypical Networks\n",
    "\n",
    "### Concept\n",
    "Instead of pre-training then adapting, **learn how to segment from K examples during training itself**. The model learns to extract good feature embeddings such that pixels from the same class (rooftop/background) cluster together.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "**Core Idea**: \n",
    "- Learn a feature encoder that maps pixels to an embedding space\n",
    "- In this space, compute class prototypes (centers) from support set\n",
    "- Classify query pixels by distance to nearest prototype\n",
    "\n",
    "**Episodic Training (on Grid 1301_11)**:\n",
    "```\n",
    "For each training episode:\n",
    "    1. Sample K images as support set\n",
    "    2. Sample Q images as query set  \n",
    "    3. Encode all images → pixel embeddings\n",
    "    4. Compute class prototypes from support set:\n",
    "       - fg_prototype = mean(embeddings where mask == 1)\n",
    "       - bg_prototype = mean(embeddings where mask == 0)\n",
    "    5. Classify query pixels by distance to prototypes\n",
    "    6. Compute loss on query predictions\n",
    "    7. Update encoder to improve few-shot segmentation\n",
    "```\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Input Image [H, W, 3]\n",
    "    ↓\n",
    "Feature Encoder (ResNet/UNet encoder)\n",
    "    ↓\n",
    "Pixel Embeddings [H, W, D]  (D = embedding dimension, e.g., 256)\n",
    "    ↓\n",
    "Prototype Computation:\n",
    "    - For each class c, compute prototype μ_c\n",
    "    - μ_c = mean of embeddings where support_mask == c\n",
    "    ↓\n",
    "Distance-based Classification:\n",
    "    - For each query pixel embedding z\n",
    "    - distance_c = ||z - μ_c||²\n",
    "    - prediction = argmin_c(distance_c)\n",
    "```\n",
    "\n",
    "**Training Details**:\n",
    "- Episodes: 1000-2000 episodes sampled from Grid 1301_11\n",
    "- K (support): 3-5 images per episode\n",
    "- Q (query): 5-10 images per episode\n",
    "- Embedding dim: 256 or 512\n",
    "- Distance metric: Euclidean or cosine\n",
    "- Loss: Cross-entropy on query predictions\n",
    "- Augmentation: Rotation, flip, color jitter, random crop\n",
    "\n",
    "**Few-Shot Inference (on Target Grids)**:\n",
    "```\n",
    "Given K labeled images from Grid 1301_13:\n",
    "    1. Encode support images → embeddings\n",
    "    2. Compute prototypes from support masks\n",
    "    3. For each query image:\n",
    "        - Encode → embeddings\n",
    "        - Classify each pixel by nearest prototype\n",
    "    4. No gradient updates needed! (zero-shot in the target domain)\n",
    "```\n",
    "\n",
    "### Advantages\n",
    "- ✅ **Explicitly designed for few-shot learning**: trained to segment from K examples\n",
    "- ✅ **No fine-tuning needed**: just compute prototypes and classify\n",
    "- ✅ **Better generalization**: learns transferable embeddings, not dataset-specific features\n",
    "- ✅ **Interpretable**: can visualize learned embeddings and prototypes\n",
    "- ✅ **Sample efficient**: should outperform fine-tuning at very low K (1-5 shots)\n",
    "\n",
    "### Limitations\n",
    "- ❌ More complex to implement than fine-tuning\n",
    "- ❌ Requires episodic training (different from standard supervised learning)\n",
    "- ❌ Slower training (need many episodes to sample diverse tasks)\n",
    "- ❌ Needs sufficient source domain diversity (Grid 1301_11 should be varied enough)\n",
    "\n",
    "### Expected Performance\n",
    "```\n",
    "Baseline (zero-shot, no adaptation):       ~XX% IoU on target grids\n",
    "Prototypical Networks with K=1:            ~XX% IoU (better than fine-tuning at K=1)\n",
    "Prototypical Networks with K=5:            ~XX% IoU (significantly better than fine-tuning)\n",
    "Prototypical Networks with K=20:           ~XX% IoU (comparable or better than fine-tuning)\n",
    "```\n",
    "\n",
    "**Key hypothesis**: Prototypical Networks should outperform fine-tuning especially at **low K (1-5 shots)** because they're explicitly trained for this scenario.\n",
    "\n",
    "### Implementation Outline (Not Implemented Yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototypical Networks pseudocode\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PrototypicalSegmentationNetwork(nn.Module):\n",
    "    def __init__(self, encoder_name='resnet34', embedding_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = build_encoder(encoder_name)  # e.g., ResNet backbone\n",
    "        self.embedding_head = nn.Conv2d(encoder_channels, embedding_dim, 1)\n",
    "    \n",
    "    def extract_embeddings(self, images):\n",
    "        # images: [B, 3, H, W]\n",
    "        features = self.encoder(images)  # [B, C, H, W]\n",
    "        embeddings = self.embedding_head(features)  # [B, D, H, W]\n",
    "        return embeddings\n",
    "    \n",
    "    def compute_prototypes(self, support_embeddings, support_masks):\n",
    "        # support_embeddings: [K, D, H, W]\n",
    "        # support_masks: [K, H, W] (binary: 0=background, 1=rooftop)\n",
    "        \n",
    "        # Flatten spatial dimensions\n",
    "        embeddings_flat = support_embeddings.flatten(2)  # [K, D, H*W]\n",
    "        masks_flat = support_masks.flatten(1)  # [K, H*W]\n",
    "        \n",
    "        # Compute prototypes for each class\n",
    "        bg_prototype = embeddings_flat[:, :, masks_flat == 0].mean(dim=-1)  # [D]\n",
    "        fg_prototype = embeddings_flat[:, :, masks_flat == 1].mean(dim=-1)  # [D]\n",
    "        \n",
    "        return torch.stack([bg_prototype, fg_prototype])  # [2, D]\n",
    "    \n",
    "    def classify_by_prototypes(self, query_embeddings, prototypes):\n",
    "        # query_embeddings: [Q, D, H, W]\n",
    "        # prototypes: [2, D]\n",
    "        \n",
    "        Q, D, H, W = query_embeddings.shape\n",
    "        embeddings_flat = query_embeddings.view(Q, D, -1).permute(0, 2, 1)  # [Q, H*W, D]\n",
    "        \n",
    "        # Compute distances to each prototype\n",
    "        distances = torch.cdist(embeddings_flat, prototypes.unsqueeze(0))  # [Q, H*W, 2]\n",
    "        \n",
    "        # Classify by nearest prototype (argmin distance)\n",
    "        predictions = distances.argmin(dim=-1).view(Q, H, W)  # [Q, H, W]\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Training loop\n",
    "model = PrototypicalSegmentationNetwork()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Sample episode from Grid 1301_11\n",
    "    support_imgs, support_masks = sample_k_images(train_by_grid['1301_11'], k=5)\n",
    "    query_imgs, query_masks = sample_q_images(train_by_grid['1301_11'], q=5)\n",
    "    \n",
    "    # Extract embeddings\n",
    "    support_emb = model.extract_embeddings(support_imgs)\n",
    "    query_emb = model.extract_embeddings(query_imgs)\n",
    "    \n",
    "    # Compute prototypes from support set\n",
    "    prototypes = model.compute_prototypes(support_emb, support_masks)\n",
    "    \n",
    "    # Classify query images\n",
    "    predictions = model.classify_by_prototypes(query_emb, prototypes)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = cross_entropy(predictions, query_masks)\n",
    "    \n",
    "    # Update encoder\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Inference on target grid\n",
    "support_imgs, support_masks = select_k_examples(train_by_grid['1301_13'], k=5)\n",
    "query_imgs, query_masks = get_remaining_images(train_by_grid['1301_13'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    support_emb = model.extract_embeddings(support_imgs)\n",
    "    query_emb = model.extract_embeddings(query_imgs)\n",
    "    \n",
    "    prototypes = model.compute_prototypes(support_emb, support_masks)\n",
    "    predictions = model.classify_by_prototypes(query_emb, prototypes)\n",
    "    \n",
    "    iou = compute_iou(predictions, query_masks)\n",
    "    print(f\"IoU on Grid 1301_13 with K=5: {iou:.3f}\")\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Approach 3: PANet (Prototype Alignment Network)\n",
    "\n",
    "### Concept\n",
    "PANet is a more sophisticated meta-learning approach specifically designed for few-shot semantic segmentation. It extends Prototypical Networks by adding **masked average pooling** and **prototype alignment** to better handle dense prediction tasks like segmentation.\n",
    "\n",
    "### Key Differences from Prototypical Networks\n",
    "While Prototypical Networks compute simple class prototypes, PANet introduces:\n",
    "1. **Masked Average Pooling (MAP)**: Uses support masks to extract more informative prototypes\n",
    "2. **Bidirectional matching**: Matches query→support AND support→query for consistency\n",
    "3. **Multi-scale features**: Combines features from multiple encoder levels\n",
    "4. **Optimized for segmentation**: Designed specifically for dense pixel-wise prediction\n",
    "\n",
    "### Methodology\n",
    "\n",
    "**Core Architecture**:\n",
    "```\n",
    "Support Set (K images + masks)           Query Image\n",
    "         ↓                                      ↓\n",
    "    Feature Encoder                       Feature Encoder\n",
    "         ↓                                      ↓\n",
    "    [K, C, H, W]                              [C, H, W]\n",
    "         ↓                                      ↓\n",
    "    Masked Average Pooling ──────────────→  Prototype Matching\n",
    "    (extract class prototypes                     ↓\n",
    "     using support masks)              Distance-based prediction\n",
    "         ↓                                      ↓\n",
    "    Foreground prototype: μ_fg          Segmentation mask [H, W]\n",
    "    Background prototype: μ_bg\n",
    "```\n",
    "\n",
    "**Masked Average Pooling (MAP)**:\n",
    "```\n",
    "For class c (foreground or background):\n",
    "    1. Collect all feature vectors from support images where mask == c\n",
    "    2. Compute prototype μ_c = mean of these feature vectors\n",
    "    3. This gives one prototype per class\n",
    "    \n",
    "μ_fg = Σ_{i,x,y} f_i(x,y) * m_i(x,y) / Σ_{i,x,y} m_i(x,y)\n",
    "μ_bg = Σ_{i,x,y} f_i(x,y) * (1-m_i(x,y)) / Σ_{i,x,y} (1-m_i(x,y))\n",
    "\n",
    "where:\n",
    "- f_i(x,y) = feature vector at position (x,y) in support image i\n",
    "- m_i(x,y) = mask value (0 or 1) at position (x,y) in support image i\n",
    "```\n",
    "\n",
    "**Prototype Alignment**:\n",
    "```\n",
    "For each query pixel embedding z:\n",
    "    1. Compute cosine similarity to each prototype\n",
    "       sim_fg = cos(z, μ_fg)\n",
    "       sim_bg = cos(z, μ_bg)\n",
    "    \n",
    "    2. Convert to prediction probability\n",
    "       P(y=fg|z) = exp(sim_fg) / (exp(sim_fg) + exp(sim_bg))\n",
    "```\n",
    "\n",
    "**Episodic Training (on Grid 1301_11)**:\n",
    "```\n",
    "For each training episode:\n",
    "    1. Sample K support images with masks\n",
    "    2. Sample Q query images with masks\n",
    "    3. Extract multi-scale features for all images\n",
    "    4. Compute class prototypes via Masked Average Pooling on support set\n",
    "    5. For each query image:\n",
    "       a. Compute similarity between query features and prototypes\n",
    "       b. Generate segmentation prediction\n",
    "    6. Compute loss (cross-entropy + auxiliary losses)\n",
    "    7. Update encoder via backpropagation\n",
    "```\n",
    "\n",
    "**Training Details**:\n",
    "- Episodes: 1000-2000 episodes from Grid 1301_11\n",
    "- K (support): 1-5 images per episode (PANet works well even with K=1)\n",
    "- Q (query): 1 image per episode (typical PANet setup)\n",
    "- Feature extractor: ResNet-50 or ResNet-101 backbone\n",
    "- Multi-scale: Uses features from multiple ResNet blocks (e.g., res3, res4, res5)\n",
    "- Loss: Cross-entropy loss on query predictions\n",
    "- Augmentation: Random crop, flip, color jitter\n",
    "\n",
    "**Few-Shot Inference (on Target Grids)**:\n",
    "```\n",
    "Given K labeled images from Grid 1301_13:\n",
    "    1. Extract features from K support images\n",
    "    2. Compute fg/bg prototypes using Masked Average Pooling\n",
    "    3. For each query image:\n",
    "       a. Extract features\n",
    "       b. Compute cosine similarity to prototypes\n",
    "       c. Generate segmentation prediction\n",
    "    4. No gradient updates! (forward pass only)\n",
    "```\n",
    "\n",
    "### Advantages\n",
    "- ✅ **State-of-art for few-shot segmentation**: Consistently outperforms simpler methods\n",
    "- ✅ **Better use of support masks**: MAP extracts richer prototypes than simple averaging\n",
    "- ✅ **Works with K=1**: Effective even with single support example\n",
    "- ✅ **Multi-scale features**: Captures both fine details and semantic context\n",
    "- ✅ **No fine-tuning needed**: Like Prototypical Networks, inference is forward-pass only\n",
    "- ✅ **Well-studied**: Multiple papers and implementations available\n",
    "\n",
    "### Limitations\n",
    "- ❌ **More complex**: Harder to implement than Prototypical Networks\n",
    "- ❌ **Computationally expensive**: Multi-scale features increase memory/compute\n",
    "- ❌ **Requires careful design**: Choosing which feature levels, how to combine them\n",
    "- ❌ **Slower training**: More complex architecture means longer training time\n",
    "- ❌ **More hyperparameters**: Feature levels, similarity metrics, loss weights\n",
    "\n",
    "### Expected Performance\n",
    "```\n",
    "Baseline (zero-shot, no adaptation):   ~XX% IoU on target grids\n",
    "PANet with K=1:                        ~(XX+5)% IoU (best at K=1)\n",
    "PANet with K=5:                        ~(XX+8)% IoU (state-of-art performance)\n",
    "PANet with K=20:                       ~(XX+10)% IoU (approaching upper bound)\n",
    "```\n",
    "\n",
    "**Key hypothesis**: PANet should outperform both fine-tuning and Prototypical Networks across all K values, with the **largest advantage at K=1-5** due to its sophisticated prototype extraction.\n",
    "\n",
    "### Implementation Outline (Not Implemented Yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANet pseudocode\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PANet(nn.Module):\n",
    "    def __init__(self, backbone='resnet50', use_multi_scale=True):\n",
    "        super().__init__()\n",
    "        self.encoder = build_resnet_encoder(backbone)  # ResNet-50 or 101\n",
    "        self.use_multi_scale = use_multi_scale\n",
    "        \n",
    "        # If using multi-scale, we'll extract features from multiple layers\n",
    "        # e.g., res3, res4, res5 from ResNet\n",
    "        if use_multi_scale:\n",
    "            self.feature_levels = ['layer2', 'layer3', 'layer4']  # ResNet layers\n",
    "        \n",
    "    def extract_features(self, images, multi_scale=False):\n",
    "        # images: [B, 3, H, W]\n",
    "        if self.use_multi_scale and multi_scale:\n",
    "            # Extract features from multiple ResNet blocks\n",
    "            features = {}\n",
    "            x = self.encoder.conv1(images)\n",
    "            x = self.encoder.bn1(x)\n",
    "            x = self.encoder.relu(x)\n",
    "            x = self.encoder.maxpool(x)\n",
    "            \n",
    "            x = self.encoder.layer1(x)\n",
    "            x = self.encoder.layer2(x)\n",
    "            features['layer2'] = x  # 1/8 resolution\n",
    "            \n",
    "            x = self.encoder.layer3(x)\n",
    "            features['layer3'] = x  # 1/16 resolution\n",
    "            \n",
    "            x = self.encoder.layer4(x)\n",
    "            features['layer4'] = x  # 1/32 resolution\n",
    "            \n",
    "            return features\n",
    "        else:\n",
    "            # Single-scale features from final layer\n",
    "            features = self.encoder(images)  # [B, C, H', W']\n",
    "            return features\n",
    "    \n",
    "    def masked_average_pooling(self, features, masks):\n",
    "        \\\"\\\"\\\"\n",
    "        Compute class prototypes using Masked Average Pooling (MAP).\n",
    "        \n",
    "        Args:\n",
    "            features: [K, C, H, W] - support features\n",
    "            masks: [K, H, W] - support masks (binary: 0=bg, 1=fg)\n",
    "        \n",
    "        Returns:\n",
    "            fg_prototype: [C] - foreground prototype\n",
    "            bg_prototype: [C] - background prototype\n",
    "        \\\"\\\"\\\"\n",
    "        K, C, H, W = features.shape\n",
    "        \n",
    "        # Resize masks to match feature resolution\n",
    "        masks_resized = F.interpolate(\n",
    "            masks.unsqueeze(1).float(), \n",
    "            size=(H, W), \n",
    "            mode='nearest'\n",
    "        ).squeeze(1)  # [K, H, W]\n",
    "        \n",
    "        # Flatten spatial dimensions\n",
    "        features_flat = features.view(K, C, -1)  # [K, C, H*W]\n",
    "        masks_flat = masks_resized.view(K, -1)  # [K, H*W]\n",
    "        \n",
    "        # Foreground prototype: average of features where mask == 1\n",
    "        fg_mask = (masks_flat == 1).unsqueeze(1)  # [K, 1, H*W]\n",
    "        fg_features = features_flat * fg_mask  # [K, C, H*W]\n",
    "        fg_count = fg_mask.sum(dim=(0, 2), keepdim=True)  # [1, 1, 1]\n",
    "        fg_prototype = fg_features.sum(dim=(0, 2)) / (fg_count.squeeze() + 1e-5)  # [C]\n",
    "        \n",
    "        # Background prototype: average of features where mask == 0\n",
    "        bg_mask = (masks_flat == 0).unsqueeze(1)  # [K, 1, H*W]\n",
    "        bg_features = features_flat * bg_mask  # [K, C, H*W]\n",
    "        bg_count = bg_mask.sum(dim=(0, 2), keepdim=True)  # [1, 1, 1]\n",
    "        bg_prototype = bg_features.sum(dim=(0, 2)) / (bg_count.squeeze() + 1e-5)  # [C]\n",
    "        \n",
    "        return fg_prototype, bg_prototype\n",
    "    \n",
    "    def prototype_alignment(self, query_features, fg_prototype, bg_prototype):\n",
    "        \\\"\\\"\\\"\n",
    "        Compute cosine similarity between query features and prototypes.\n",
    "        \n",
    "        Args:\n",
    "            query_features: [Q, C, H, W]\n",
    "            fg_prototype: [C]\n",
    "            bg_prototype: [C]\n",
    "        \n",
    "        Returns:\n",
    "            predictions: [Q, 2, H, W] - similarity scores for each class\n",
    "        \\\"\\\"\\\"\n",
    "        Q, C, H, W = query_features.shape\n",
    "        \n",
    "        # Normalize features and prototypes for cosine similarity\n",
    "        query_norm = F.normalize(query_features, p=2, dim=1)  # [Q, C, H, W]\n",
    "        fg_proto_norm = F.normalize(fg_prototype.unsqueeze(0), p=2, dim=1)  # [1, C]\n",
    "        bg_proto_norm = F.normalize(bg_prototype.unsqueeze(0), p=2, dim=1)  # [1, C]\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        # query_norm: [Q, C, H, W], proto_norm: [1, C] -> need to broadcast\n",
    "        fg_sim = (query_norm * fg_proto_norm.view(1, C, 1, 1)).sum(dim=1)  # [Q, H, W]\n",
    "        bg_sim = (query_norm * bg_proto_norm.view(1, C, 1, 1)).sum(dim=1)  # [Q, H, W]\n",
    "        \n",
    "        # Stack similarities for both classes\n",
    "        similarity_map = torch.stack([bg_sim, fg_sim], dim=1)  # [Q, 2, H, W]\n",
    "        \n",
    "        return similarity_map\n",
    "    \n",
    "    def forward(self, support_images, support_masks, query_images):\n",
    "        \\\"\\\"\\\"\n",
    "        PANet forward pass for few-shot segmentation.\n",
    "        \n",
    "        Args:\n",
    "            support_images: [K, 3, H, W]\n",
    "            support_masks: [K, H, W]\n",
    "            query_images: [Q, 3, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            predictions: [Q, 2, H, W] - class scores for query images\n",
    "        \\\"\\\"\\\"\n",
    "        # Extract features\n",
    "        support_features = self.extract_features(support_images)  # [K, C, H', W']\n",
    "        query_features = self.extract_features(query_images)  # [Q, C, H', W']\n",
    "        \n",
    "        # Compute prototypes from support set\n",
    "        fg_prototype, bg_prototype = self.masked_average_pooling(\n",
    "            support_features, support_masks\n",
    "        )\n",
    "        \n",
    "        # Align query features with prototypes\n",
    "        similarity_map = self.prototype_alignment(\n",
    "            query_features, fg_prototype, bg_prototype\n",
    "        )\n",
    "        \n",
    "        # Upsample to original resolution\n",
    "        Q, _, H, W = query_images.shape\n",
    "        predictions = F.interpolate(\n",
    "            similarity_map,\n",
    "            size=(H, W),\n",
    "            mode='bilinear',\n",
    "            align_corners=True\n",
    "        )  # [Q, 2, H, W]\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Training loop\n",
    "model = PANet(backbone='resnet50')\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Sample episode from Grid 1301_11\n",
    "    support_imgs, support_masks = sample_k_images(train_by_grid['1301_11'], k=5)\n",
    "    query_imgs, query_masks = sample_q_images(train_by_grid['1301_11'], q=1)\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = model(support_imgs, support_masks, query_imgs)  # [Q, 2, H, W]\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.cross_entropy(predictions, query_masks.long())\n",
    "    \n",
    "    # Update model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Inference on target grid\n",
    "support_imgs, support_masks = select_k_examples(train_by_grid['1301_13'], k=5)\n",
    "query_imgs, query_masks = get_remaining_images(train_by_grid['1301_13'])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(support_imgs, support_masks, query_imgs)  # [Q, 2, H, W]\n",
    "    pred_masks = predictions.argmax(dim=1)  # [Q, H, W]\n",
    "    \n",
    "    iou = compute_iou(pred_masks, query_masks)\n",
    "    print(f\"IoU on Grid 1301_13 with K=5: {iou:.3f}\")\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Comparison of Approaches\n",
    "\n",
    "| Aspect | Fine-Tuning | Prototypical Networks | PANet |\n",
    "|--------|-------------|----------------------|-------|\n",
    "| **Training paradigm** | Standard supervised → fine-tune | Episodic meta-learning | Episodic meta-learning |\n",
    "| **Adaptation method** | Gradient updates on K examples | Compute prototypes (no updates) | Compute prototypes (no updates) |\n",
    "| **Prototype extraction** | N/A | Simple averaging | Masked Average Pooling (MAP) |\n",
    "| **Feature representation** | Task-specific | Learned embeddings | Multi-scale learned embeddings |\n",
    "| **Sample efficiency** | Moderate (needs K~10-20) | High (works well with K=3-5) | Very high (works well with K=1-3) |\n",
    "| **Overfitting risk** | High at low K | Lower | Lowest |\n",
    "| **Computational cost (training)** | Low (few gradient steps) | Medium (episodic training) | High (complex architecture) |\n",
    "| **Computational cost (inference)** | Low | Low | Medium (multi-scale features) |\n",
    "| **Implementation complexity** | Simple | Moderate | High |\n",
    "| **Interpretability** | Standard CNN features | Explicit embeddings + prototypes | Prototypes + similarity maps |\n",
    "| **Expected performance K=1** | Poor | Moderate | Good |\n",
    "| **Expected performance K=5** | Moderate | Good | Very good |\n",
    "| **Expected performance K=20** | Good | Very good | Excellent |\n",
    "\n",
    "### Detailed Comparison\n",
    "\n",
    "#### 1. **Fine-Tuning (Baseline)**\n",
    "- **Best for**: K > 10, similar domains, simple baseline\n",
    "- **Strengths**: Easy to implement, well-understood, works with any architecture\n",
    "- **Weaknesses**: Overfits at low K, requires hyperparameter tuning\n",
    "- **Use case**: When you have enough target data and want a simple solution\n",
    "\n",
    "#### 2. **Prototypical Networks**\n",
    "- **Best for**: K = 3-10, learning transferable embeddings\n",
    "- **Strengths**: Designed for few-shot, no fine-tuning needed, interpretable\n",
    "- **Weaknesses**: Simpler than PANet, may not capture complex patterns\n",
    "- **Use case**: Balance between performance and implementation complexity\n",
    "\n",
    "#### 3. **PANet (State-of-Art)**\n",
    "- **Best for**: K = 1-5, maximum performance, complex domain shifts\n",
    "- **Strengths**: Best performance, works with K=1, sophisticated prototype extraction\n",
    "- **Weaknesses**: Complex to implement, computationally expensive, many hyperparameters\n",
    "- **Use case**: When you need the best possible performance and can handle complexity\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "| Scenario | Recommended Method | Rationale |\n",
    "|----------|-------------------|-----------|\n",
    "| K = 1 shot | **PANet** | Only PANet handles single example well |\n",
    "| K = 3-5 shots | **PANet or Prototypical** | Both work well, trade complexity for performance |\n",
    "| K = 10+ shots | **Fine-tuning or Prototypical** | Diminishing returns from PANet complexity |\n",
    "| Tutorial/Educational | **Prototypical Networks** | Best balance of concepts and implementation |\n",
    "| Production deployment | **PANet** | Maximum performance, worth the engineering |\n",
    "| Quick baseline | **Fine-tuning** | Simplest, fastest to implement |\n",
    "| Large domain shift | **PANet** | Best generalization to new domains |\n",
    "| Limited compute | **Fine-tuning** | Lowest computational requirements |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Evaluation Protocol\n",
    "\n",
    "For all three approaches, we'll use the same evaluation protocol:\n",
    "\n",
    "### 5.1 Metrics\n",
    "- **Intersection over Union (IoU)**: Primary metric for segmentation quality\n",
    "- **Pixel Accuracy**: Overall percentage of correctly classified pixels\n",
    "- **Dice Coefficient**: Harmonic mean of precision and recall\n",
    "- **F1-Score**: Per-class performance metric\n",
    "\n",
    "### 5.2 Experimental Setup\n",
    "```\n",
    "For K in [1, 3, 5, 10, 20]:\n",
    "    For target_grid in ['1301_13', '1301_31']:\n",
    "        For trial in range(5):  # 5 random trials for statistical significance\n",
    "            # Randomly select K support examples\n",
    "            support_set = random.sample(target_grid_data, K)\n",
    "            query_set = remaining images\n",
    "            \n",
    "            # Apply each method\n",
    "            pred_finetuning = fine_tuning_method.predict(support_set, query_set)\n",
    "            pred_prototypical = prototypical_method.predict(support_set, query_set)\n",
    "            pred_panet = panet_method.predict(support_set, query_set)\n",
    "            \n",
    "            # Compute metrics for each\n",
    "            iou_ft = compute_iou(pred_finetuning, query_ground_truth)\n",
    "            iou_proto = compute_iou(pred_prototypical, query_ground_truth)\n",
    "            iou_panet = compute_iou(pred_panet, query_ground_truth)\n",
    "            \n",
    "        # Report mean ± std across trials\n",
    "```\n",
    "\n",
    "### 5.3 Baselines\n",
    "1. **Zero-shot**: Trained on 1301_11, test directly on target (no adaptation)\n",
    "2. **Upper bound**: Train on full target grid data (supervised learning)\n",
    "3. **Random**: Random segmentation (sanity check)\n",
    "\n",
    "### 5.4 Statistical Testing\n",
    "- **Paired t-test**: Compare methods at each K value\n",
    "- **Confidence intervals**: 95% CI for mean IoU\n",
    "- **Significance level**: α = 0.05\n",
    "\n",
    "### 5.5 Visualization\n",
    "- **Performance vs. K curves**: Line plot showing all three methods\n",
    "- **Box plots**: Distribution of IoU scores across trials\n",
    "- **Qualitative results**: Side-by-side segmentation outputs\n",
    "- **Error analysis**: Where does each method fail?\n",
    "- **Prototype visualization** (for Prototypical Networks and PANet):\n",
    "  - t-SNE of learned embeddings\n",
    "  - Similarity maps\n",
    "  - Prototype evolution as K increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation utilities (to be implemented)\n",
    "\"\"\"\n",
    "def compute_iou(predictions, targets, num_classes=2):\n",
    "    \\\"\\\"\\\"\n",
    "    Compute intersection over union for each class.\n",
    "    \n",
    "    Args:\n",
    "        predictions: [N, H, W] - predicted masks\n",
    "        targets: [N, H, W] - ground truth masks\n",
    "        num_classes: number of classes\n",
    "    \n",
    "    Returns:\n",
    "        iou: mean IoU across classes\n",
    "    \\\"\\\"\\\"\n",
    "    pass\n",
    "\n",
    "def evaluate_fewshot(model, support_set, query_set, method='prototypical'):\n",
    "    \\\"\\\"\\\"\n",
    "    Run few-shot evaluation for a given method.\n",
    "    \n",
    "    Args:\n",
    "        model: trained model\n",
    "        support_set: K labeled examples\n",
    "        query_set: unlabeled test examples\n",
    "        method: 'fine_tuning', 'prototypical', or 'panet'\n",
    "    \n",
    "    Returns:\n",
    "        metrics: dict with IoU, Dice, accuracy, etc.\n",
    "    \\\"\\\"\\\"\n",
    "    pass\n",
    "\n",
    "def plot_performance_curve(results_dict, save_path=None):\n",
    "    \\\"\\\"\\\"\n",
    "    Plot IoU vs K for all three methods.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: {\n",
    "            'fine_tuning': {K: [iou_values]}, \n",
    "            'prototypical': {K: [iou_values]},\n",
    "            'panet': {K: [iou_values]}\n",
    "        }\n",
    "    \\\"\\\"\\\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for method_name, results in results_dict.items():\n",
    "        K_values = sorted(results.keys())\n",
    "        mean_ious = [np.mean(results[k]) for k in K_values]\n",
    "        std_ious = [np.std(results[k]) for k in K_values]\n",
    "        \n",
    "        ax.plot(K_values, mean_ious, marker='o', label=method_name)\n",
    "        ax.fill_between(K_values, \n",
    "                        np.array(mean_ious) - np.array(std_ious),\n",
    "                        np.array(mean_ious) + np.array(std_ious),\n",
    "                        alpha=0.2)\n",
    "    \n",
    "    ax.set_xlabel('K (number of support examples)')\n",
    "    ax.set_ylabel('Mean IoU')\n",
    "    ax.set_title('Few-Shot Segmentation Performance')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    pass\n",
    "\n",
    "def visualize_predictions(images, masks, predictions_dict, n_samples=5):\n",
    "    \\\"\\\"\\\"\n",
    "    Side-by-side comparison of ground truth and all method predictions.\n",
    "    \n",
    "    Args:\n",
    "        images: [N, H, W, 3] - input images\n",
    "        masks: [N, H, W] - ground truth masks\n",
    "        predictions_dict: {\n",
    "            'fine_tuning': [N, H, W],\n",
    "            'prototypical': [N, H, W],\n",
    "            'panet': [N, H, W]\n",
    "        }\n",
    "        n_samples: number of samples to visualize\n",
    "    \\\"\\\"\\\"\n",
    "    pass\n",
    "\n",
    "def visualize_prototypes(model, support_set, method='prototypical'):\n",
    "    \\\"\\\"\\\"\n",
    "    Visualize learned embeddings and prototypes.\n",
    "    \n",
    "    For Prototypical Networks and PANet:\n",
    "    - t-SNE visualization of pixel embeddings\n",
    "    - Show foreground/background prototype locations\n",
    "    - Similarity maps for query images\n",
    "    \\\"\\\"\\\"\n",
    "    pass\n",
    "\n",
    "def statistical_comparison(results_dict, K_value=5):\n",
    "    \\\"\\\"\\\"\n",
    "    Perform paired t-test between methods at a specific K.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: results from all methods\n",
    "        K_value: which K to compare\n",
    "    \n",
    "    Returns:\n",
    "        p_values: pairwise p-values\n",
    "    \\\"\\\"\\\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    methods = list(results_dict.keys())\n",
    "    p_values = {}\n",
    "    \n",
    "    for i, method1 in enumerate(methods):\n",
    "        for method2 in methods[i+1:]:\n",
    "            ious1 = results_dict[method1][K_value]\n",
    "            ious2 = results_dict[method2][K_value]\n",
    "            \n",
    "            t_stat, p_val = stats.ttest_rel(ious1, ious2)\n",
    "            p_values[f'{method1}_vs_{method2}'] = p_val\n",
    "    \n",
    "    return p_values\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Cross-City Transfer: Integrating Inria Aerial Dataset\n",
    "\n",
    "### Motivation\n",
    "To make the few-shot learning scenario more realistic and demonstrate the full power of meta-learning approaches, we integrate the **Inria Aerial Image Dataset** as cross-city target domains. This tests true **cross-city generalization**, moving beyond within-city transfer to real-world deployment scenarios.\n",
    "\n",
    "### Why Inria Aerial Dataset?\n",
    "\n",
    "**Dataset Overview**:\n",
    "- **5 cities**: Austin (US), Chicago (US), Kitsap (US), Vienna (Austria), Tyrol (Austria)\n",
    "- **Coverage**: 180 km² total across all cities\n",
    "- **Images**: 360 images (180 train + 180 test), 5000×5000 pixels each\n",
    "- **Resolution**: 0.3m per pixel\n",
    "- **Task**: Building footprint segmentation (binary: building vs background)\n",
    "- **Availability**: Free, well-documented benchmark dataset\n",
    "- **HuggingFace**: `Jonathan/INRIA-Aerial-Dataset`\n",
    "\n",
    "**Why This Strategy (Geneva + Inria)**:\n",
    "✅ **Progressive difficulty**: Start with small domain shift (Geneva grids), then cross-city transfer (Inria cities)\n",
    "✅ **Same task throughout**: Binary segmentation (rooftop/building vs background)\n",
    "✅ **Educational value**: Clear learning progression from easy to hard\n",
    "✅ **Multiple test scenarios**: 2 Geneva grids + up to 5 Inria cities\n",
    "✅ **Real-world relevance**: \"Train on Geneva, deploy to Vienna\" mimics actual government use cases\n",
    "\n",
    "**Expected Domain Shifts**:\n",
    "```\n",
    "Geneva 1301_11 → Geneva 1301_13:  SMALL (same city, different neighborhood)\n",
    "Geneva 1301_11 → Vienna:          MEDIUM (both European, different city)\n",
    "Geneva 1301_11 → Austin:          LARGE (European → US, different architecture)\n",
    "Geneva 1301_11 → Kitsap:          VERY LARGE (European urban → US rural)\n",
    "```\n",
    "\n",
    "### Recommended Implementation: Vienna + Austin\n",
    "\n",
    "For this tutorial, we'll focus on **two Inria cities**:\n",
    "\n",
    "1. **Vienna** (Medium shift)\n",
    "   - European city like Geneva\n",
    "   - Moderate density, similar coordinate system\n",
    "   - Different architecture but same continental style\n",
    "   - Expected: Meta-learning should show moderate advantage\n",
    "\n",
    "2. **Austin** (Large shift)\n",
    "   - US suburban sprawl\n",
    "   - Low-density, very different building types\n",
    "   - Large domain gap from Geneva\n",
    "   - Expected: Meta-learning should show large advantage over fine-tuning\n",
    "\n",
    "### Integration Workflow\n",
    "\n",
    "#### Step 1: Download and Prepare Inria Dataset\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def download_inria_dataset(cities=['vienna', 'austin']):\n",
    "    \"\"\"\n",
    "    Download Inria dataset from HuggingFace.\n",
    "    \n",
    "    Args:\n",
    "        cities: List of cities to download\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with city data\n",
    "    \"\"\"\n",
    "    # Load full dataset\n",
    "    inria_data = load_dataset(\"Jonathan/INRIA-Aerial-Dataset\")\n",
    "    \n",
    "    # Filter by cities\n",
    "    city_data = {}\n",
    "    for city in cities:\n",
    "        city_data[city] = {\n",
    "            'train': inria_data['train'].filter(lambda x: x['city'].lower() == city),\n",
    "            'test': inria_data['test'].filter(lambda x: x['city'].lower() == city)\n",
    "        }\n",
    "    \n",
    "    return city_data\n",
    "```\n",
    "\n",
    "#### Step 2: Tile Images to Match Geneva Format\n",
    "\n",
    "```python\n",
    "def tile_image(image, mask, tile_size=250, stride=250):\n",
    "    \"\"\"\n",
    "    Tile large Inria images (5000x5000) into Geneva-compatible patches (250x250).\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or numpy array [H, W, 3]\n",
    "        mask: PIL Image or numpy array [H, W]\n",
    "        tile_size: Size of output tiles (default 250 to match Geneva)\n",
    "        stride: Stride for tiling (default 250 for non-overlapping)\n",
    "    \n",
    "    Returns:\n",
    "        List of (image_tile, mask_tile) tuples\n",
    "    \"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "    if isinstance(mask, Image.Image):\n",
    "        mask = np.array(mask)\n",
    "    \n",
    "    H, W = image.shape[:2]\n",
    "    tiles = []\n",
    "    \n",
    "    for y in range(0, H - tile_size + 1, stride):\n",
    "        for x in range(0, W - tile_size + 1, stride):\n",
    "            img_tile = image[y:y+tile_size, x:x+tile_size]\n",
    "            mask_tile = mask[y:y+tile_size, x:x+tile_size]\n",
    "            \n",
    "            # Skip tiles with no buildings (optional: for efficiency)\n",
    "            if mask_tile.sum() > 0:  # At least some building pixels\n",
    "                tiles.append((img_tile, mask_tile))\n",
    "    \n",
    "    return tiles\n",
    "\n",
    "def preprocess_inria_city(city_data, output_dir, tile_size=250):\n",
    "    \"\"\"\n",
    "    Process all images from one Inria city.\n",
    "    \n",
    "    Args:\n",
    "        city_data: Dataset for one city (train/test splits)\n",
    "        output_dir: Where to save tiled images\n",
    "        tile_size: Tile size (default 250)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with paths to tiled images\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    tiled_data = {'train': [], 'test': []}\n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        for idx, sample in enumerate(city_data[split]):\n",
    "            image = sample['image']  # PIL Image\n",
    "            mask = sample['mask']    # PIL Image (building=1, background=0)\n",
    "            \n",
    "            # Tile into 250x250 patches\n",
    "            tiles = tile_image(image, mask, tile_size=tile_size)\n",
    "            \n",
    "            # Save tiles\n",
    "            for tile_idx, (img_tile, mask_tile) in enumerate(tiles):\n",
    "                img_path = output_dir / split / 'images' / f\"{idx}_{tile_idx}.png\"\n",
    "                mask_path = output_dir / split / 'masks' / f\"{idx}_{tile_idx}.png\"\n",
    "                \n",
    "                img_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                mask_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                Image.fromarray(img_tile).save(img_path)\n",
    "                Image.fromarray(mask_tile).save(mask_path)\n",
    "                \n",
    "                tiled_data[split].append((str(img_path), str(mask_path)))\n",
    "    \n",
    "    print(f\"Created {len(tiled_data['train'])} train tiles and {len(tiled_data['test'])} test tiles\")\n",
    "    return tiled_data\n",
    "```\n",
    "\n",
    "#### Step 3: Normalize and Align Preprocessing\n",
    "\n",
    "```python\n",
    "def analyze_dataset_statistics(dataset_paths):\n",
    "    \"\"\"\n",
    "    Compute mean and std of RGB channels for normalization.\n",
    "    \n",
    "    Args:\n",
    "        dataset_paths: List of (image_path, mask_path) tuples\n",
    "    \n",
    "    Returns:\n",
    "        mean, std: RGB channel statistics\n",
    "    \"\"\"\n",
    "    means = []\n",
    "    stds = []\n",
    "    \n",
    "    for img_path, _ in dataset_paths[:100]:  # Sample 100 images\n",
    "        img = np.array(Image.open(img_path)) / 255.0\n",
    "        means.append(img.mean(axis=(0, 1)))\n",
    "        stds.append(img.std(axis=(0, 1)))\n",
    "    \n",
    "    mean = np.mean(means, axis=0)\n",
    "    std = np.mean(stds, axis=0)\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# Compare Geneva vs Inria statistics\n",
    "geneva_mean, geneva_std = analyze_dataset_statistics(train_by_grid['1301_11'])\n",
    "vienna_mean, vienna_std = analyze_dataset_statistics(vienna_tiled_data['train'])\n",
    "austin_mean, austin_std = analyze_dataset_statistics(austin_tiled_data['train'])\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Geneva: mean={geneva_mean}, std={geneva_std}\")\n",
    "print(f\"Vienna: mean={vienna_mean}, std={vienna_std}\")\n",
    "print(f\"Austin: mean={austin_mean}, std={austin_std}\")\n",
    "```\n",
    "\n",
    "#### Step 4: Three-Level Evaluation Framework\n",
    "\n",
    "```\n",
    "Tutorial Evaluation Structure:\n",
    "\n",
    "Level 1: Within-City Transfer (Baseline)\n",
    "    Train: Geneva Grid 1301_11\n",
    "    Test:  Geneva Grids 1301_13, 1301_31\n",
    "    Purpose: Validate methods, small domain shift\n",
    "    Expected: All methods work reasonably well\n",
    "\n",
    "Level 2: Cross-City Transfer - Similar Domain (Vienna)\n",
    "    Train: Geneva Grid 1301_11\n",
    "    Test:  Vienna (Inria)\n",
    "    Purpose: Medium domain shift, European → European\n",
    "    Expected: Meta-learning starts showing advantage\n",
    "\n",
    "Level 3: Cross-City Transfer - Large Domain Shift (Austin)\n",
    "    Train: Geneva Grid 1301_11\n",
    "    Test:  Austin (Inria)\n",
    "    Purpose: Large domain shift, European urban → US suburban\n",
    "    Expected: Meta-learning shows large advantage, fine-tuning struggles\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inria dataset integration implementation (to be completed)\n",
    "\"\"\"\n",
    "# Complete implementation for integrating Inria dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Step 1: Download Inria dataset\n",
    "def load_and_prepare_inria(cities=['vienna', 'austin'], cache_dir='./data/inria'):\n",
    "    \\\"\\\"\\\"\n",
    "    Download and prepare Inria dataset for cross-city evaluation.\n",
    "    \\\"\\\"\\\"\n",
    "    print(f\"Downloading Inria dataset for cities: {cities}\")\n",
    "    \n",
    "    # Load from HuggingFace\n",
    "    inria_dataset = load_dataset(\"Jonathan/INRIA-Aerial-Dataset\", cache_dir=cache_dir)\n",
    "    \n",
    "    city_data = {}\n",
    "    for city in cities:\n",
    "        print(f\"Processing {city}...\")\n",
    "        \n",
    "        # Filter by city (case-insensitive)\n",
    "        train_city = [x for x in inria_dataset['train'] if x['city'].lower() == city.lower()]\n",
    "        test_city = [x for x in inria_dataset['test'] if x['city'].lower() == city.lower()]\n",
    "        \n",
    "        city_data[city] = {\n",
    "            'train': train_city,\n",
    "            'test': test_city\n",
    "        }\n",
    "        \n",
    "        print(f\"  {city}: {len(train_city)} train, {len(test_city)} test images\")\n",
    "    \n",
    "    return city_data\n",
    "\n",
    "# Step 2: Tile images to 250x250\n",
    "def tile_inria_images(city_data, output_dir, tile_size=250, stride=250, min_building_pixels=100):\n",
    "    \\\"\\\"\\\"\n",
    "    Tile Inria 5000x5000 images into 250x250 patches compatible with Geneva.\n",
    "    \n",
    "    Args:\n",
    "        city_data: Dictionary with train/test splits for a city\n",
    "        output_dir: Where to save tiles\n",
    "        tile_size: Size of each tile (default 250)\n",
    "        stride: Stride for tiling (default 250 for non-overlapping)\n",
    "        min_building_pixels: Minimum building pixels to keep a tile\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with tile paths: {'train': [(img, mask), ...], 'test': [...]}\n",
    "    \\\"\\\"\\\"\n",
    "    output_dir = Path(output_dir)\n",
    "    tiled_paths = {'train': [], 'test': []}\n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        split_dir = output_dir / split\n",
    "        (split_dir / 'images').mkdir(parents=True, exist_ok=True)\n",
    "        (split_dir / 'masks').mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for img_idx, sample in enumerate(city_data[split]):\n",
    "            # Get image and mask (already PIL Images)\n",
    "            image = sample['image']  # 5000x5000 RGB\n",
    "            mask = sample['mask']    # 5000x5000 binary\n",
    "            \n",
    "            # Convert to numpy for tiling\n",
    "            img_np = np.array(image)\n",
    "            mask_np = np.array(mask)\n",
    "            \n",
    "            H, W = img_np.shape[:2]\n",
    "            tile_idx = 0\n",
    "            \n",
    "            # Tile the image\n",
    "            for y in range(0, H - tile_size + 1, stride):\n",
    "                for x in range(0, W - tile_size + 1, stride):\n",
    "                    img_tile = img_np[y:y+tile_size, x:x+tile_size]\n",
    "                    mask_tile = mask_np[y:y+tile_size, x:x+tile_size]\n",
    "                    \n",
    "                    # Skip tiles with too few building pixels\n",
    "                    if mask_tile.sum() < min_building_pixels:\n",
    "                        continue\n",
    "                    \n",
    "                    # Save tiles\n",
    "                    img_path = split_dir / 'images' / f\"img{img_idx:03d}_tile{tile_idx:03d}.png\"\n",
    "                    mask_path = split_dir / 'masks' / f\"img{img_idx:03d}_tile{tile_idx:03d}.png\"\n",
    "                    \n",
    "                    Image.fromarray(img_tile).save(img_path)\n",
    "                    Image.fromarray(mask_tile).save(mask_path)\n",
    "                    \n",
    "                    tiled_paths[split].append((str(img_path), str(mask_path)))\n",
    "                    tile_idx += 1\n",
    "        \n",
    "        print(f\"{split}: Created {len(tiled_paths[split])} tiles\")\n",
    "    \n",
    "    return tiled_paths\n",
    "\n",
    "# Step 3: Cross-city evaluation\n",
    "def evaluate_cross_city_transfer(model, geneva_train, vienna_data, austin_data, K_values=[1, 3, 5, 10, 20]):\n",
    "    \\\"\\\"\\\"\n",
    "    Evaluate few-shot transfer from Geneva to Vienna and Austin.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model (Fine-tuning, Prototypical, or PANet)\n",
    "        geneva_train: Geneva Grid 1301_11 data (source domain)\n",
    "        vienna_data: Vienna tiled data (target domain 1)\n",
    "        austin_data: Austin tiled data (target domain 2)\n",
    "        K_values: List of K values to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results for each target city and K value\n",
    "    \\\"\\\"\\\"\n",
    "    results = {\n",
    "        'vienna': {},\n",
    "        'austin': {}\n",
    "    }\n",
    "    \n",
    "    # Test on Vienna\n",
    "    print(\"\\\\n=== Vienna Cross-City Transfer ===\")\n",
    "    for K in K_values:\n",
    "        # Sample K support examples from Vienna\n",
    "        support_vienna = random.sample(vienna_data['test'], K)\n",
    "        query_vienna = [x for x in vienna_data['test'] if x not in support_vienna]\n",
    "        \n",
    "        # Run few-shot evaluation\n",
    "        iou_vienna = model.evaluate_fewshot(support_vienna, query_vienna)\n",
    "        results['vienna'][K] = iou_vienna\n",
    "        \n",
    "        print(f\"K={K}: IoU = {iou_vienna:.3f}\")\n",
    "    \n",
    "    # Test on Austin\n",
    "    print(\"\\\\n=== Austin Cross-City Transfer ===\")\n",
    "    for K in K_values:\n",
    "        # Sample K support examples from Austin\n",
    "        support_austin = random.sample(austin_data['test'], K)\n",
    "        query_austin = [x for x in austin_data['test'] if x not in support_austin]\n",
    "        \n",
    "        # Run few-shot evaluation\n",
    "        iou_austin = model.evaluate_fewshot(support_austin, query_austin)\n",
    "        results['austin'][K] = iou_austin\n",
    "        \n",
    "        print(f\"K={K}: IoU = {iou_austin:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Step 4: Visualization - Compare within-city vs cross-city\n",
    "def plot_cross_city_comparison(results_geneva, results_vienna, results_austin, save_path=None):\n",
    "    \\\"\\\"\\\"\n",
    "    Plot performance comparison across different domain shifts.\n",
    "    \n",
    "    Args:\n",
    "        results_geneva: Results on Geneva grids (1301_13, 1301_31)\n",
    "        results_vienna: Results on Vienna\n",
    "        results_austin: Results on Austin\n",
    "    \\\"\\\"\\\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Fine-tuning across domains\n",
    "    K_values = sorted(results_geneva['fine_tuning'].keys())\n",
    "    \n",
    "    ax1.plot(K_values, [results_geneva['fine_tuning'][k] for k in K_values], \n",
    "             marker='o', label='Geneva (small shift)', linewidth=2)\n",
    "    ax1.plot(K_values, [results_vienna['fine_tuning'][k] for k in K_values], \n",
    "             marker='s', label='Vienna (medium shift)', linewidth=2)\n",
    "    ax1.plot(K_values, [results_austin['fine_tuning'][k] for k in K_values], \n",
    "             marker='^', label='Austin (large shift)', linewidth=2)\n",
    "    \n",
    "    ax1.set_xlabel('K (number of support examples)', fontsize=12)\n",
    "    ax1.set_ylabel('Mean IoU', fontsize=12)\n",
    "    ax1.set_title('Fine-Tuning: Performance vs Domain Shift', fontsize=14)\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: PANet across domains (should be more robust)\n",
    "    ax2.plot(K_values, [results_geneva['panet'][k] for k in K_values], \n",
    "             marker='o', label='Geneva (small shift)', linewidth=2)\n",
    "    ax2.plot(K_values, [results_vienna['panet'][k] for k in K_values], \n",
    "             marker='s', label='Vienna (medium shift)', linewidth=2)\n",
    "    ax2.plot(K_values, [results_austin['panet'][k] for k in K_values], \n",
    "             marker='^', label='Austin (large shift)', linewidth=2)\n",
    "    \n",
    "    ax2.set_xlabel('K (number of support examples)', fontsize=12)\n",
    "    ax2.set_ylabel('Mean IoU', fontsize=12)\n",
    "    ax2.set_title('PANet: Performance vs Domain Shift', fontsize=14)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Usage example:\n",
    "# city_data = load_and_prepare_inria(cities=['vienna', 'austin'])\n",
    "# vienna_tiles = tile_inria_images(city_data['vienna'], output_dir='data/inria/vienna')\n",
    "# austin_tiles = tile_inria_images(city_data['austin'], output_dir='data/inria/austin')\n",
    "# results = evaluate_cross_city_transfer(model, geneva_train, vienna_tiles, austin_tiles)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Recommended Starting Points for Implementation\n",
    "\n",
    "### For This Tutorial Specifically\n",
    "\n",
    "Based on our rooftop segmentation task, here are the most relevant resources:\n",
    "\n",
    "#### 1. **Start with PFENet for Understanding** ⭐ RECOMMENDED\n",
    "   ```bash\n",
    "   git clone https://github.com/dvlab-research/PFENet\n",
    "   ```\n",
    "   **Why**: \n",
    "   - Cleanest PyTorch implementation\n",
    "   - Well-documented code structure\n",
    "   - Easy to adapt for binary segmentation\n",
    "   - Good episodic training examples\n",
    "   \n",
    "   **Use for**: Understanding episodic training loop and prototype computation\n",
    "\n",
    "#### 2. **Study PANet Official Implementation**\n",
    "   ```bash\n",
    "   git clone https://github.com/kaixin96/PANet\n",
    "   ```\n",
    "   **Why**:\n",
    "   - Official implementation\n",
    "   - Complete masked average pooling code\n",
    "   - Even though it's TensorFlow, the logic is clear\n",
    "   \n",
    "   **Use for**: Understanding PANet architecture details\n",
    "\n",
    "#### 3. **Use HSNet for PyTorch PANet-style Implementation**\n",
    "   ```bash\n",
    "   git clone https://github.com/juhongm999/hsnet\n",
    "   ```\n",
    "   **Why**:\n",
    "   - Modern PyTorch implementation\n",
    "   - Similar to PANet but better documented\n",
    "   - Can adapt their prototype extraction code\n",
    "   \n",
    "   **Use for**: Adapting masked average pooling to our dataset\n",
    "\n",
    "#### 4. **Leverage segmentation_models.pytorch for Baseline**\n",
    "   ```bash\n",
    "   pip install segmentation-models-pytorch\n",
    "   ```\n",
    "   **Why**:\n",
    "   - Pre-built U-Net, DeepLabV3 models\n",
    "   - Easy to use for fine-tuning baseline\n",
    "   - Lots of pre-trained encoders (ResNet, EfficientNet)\n",
    "   \n",
    "   **Use for**: Quick baseline implementation\n",
    "\n",
    "#### 5. **Use learn2learn for Prototypical Networks**\n",
    "   ```bash\n",
    "   pip install learn2learn\n",
    "   ```\n",
    "   **Why**:\n",
    "   - High-level API for meta-learning\n",
    "   - Built-in episodic data loaders\n",
    "   - Prototypical Networks already implemented\n",
    "   \n",
    "   **Use for**: Prototypical Networks implementation\n",
    "\n",
    "### Suggested Code Reuse Strategy\n",
    "\n",
    "#### Phase 1: Baseline (Fine-tuning)\n",
    "```python\n",
    "# Use segmentation_models.pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=2\n",
    ")\n",
    "\n",
    "# Train on Grid 1301_11\n",
    "# Fine-tune on K examples from target grid\n",
    "```\n",
    "\n",
    "#### Phase 2: Prototypical Networks\n",
    "```python\n",
    "# Adapt from PFENet or learn2learn\n",
    "# Reference: https://github.com/dvlab-research/PFENet/blob/master/model/PFENet.py\n",
    "\n",
    "# Key components to adapt:\n",
    "# 1. Episodic data loader (sample K-shot tasks)\n",
    "# 2. Prototype computation (masked average pooling)\n",
    "# 3. Distance-based classification\n",
    "```\n",
    "\n",
    "#### Phase 3: PANet\n",
    "```python\n",
    "# Adapt from HSNet\n",
    "# Reference: https://github.com/juhongm999/hsnet/blob/main/model/hsnet.py\n",
    "\n",
    "# Key components to adapt:\n",
    "# 1. Multi-scale feature extraction\n",
    "# 2. Masked average pooling (more sophisticated than Prototypical)\n",
    "# 3. Cosine similarity matching\n",
    "```\n",
    "\n",
    "### Recommended External Dataset\n",
    "\n",
    "For cross-city evaluation, use **Inria Aerial Dataset (Vienna)**:\n",
    "\n",
    "```python\n",
    "# Available on HuggingFace\n",
    "from datasets import load_dataset\n",
    "\n",
    "inria = load_dataset(\"Jonathan/INRIA-Aerial-Dataset\", split=\"train\")\n",
    "\n",
    "# Filter for Vienna\n",
    "vienna_data = inria.filter(lambda x: x['city'] == 'vienna')\n",
    "```\n",
    "\n",
    "**Why Vienna**:\n",
    "- Similar to Geneva (European city)\n",
    "- Same coordinate system (will look similar visually)\n",
    "- But different architecture/urban layout\n",
    "- Perfect for testing domain adaptation\n",
    "\n",
    "**Alternative**: Use **Austin** from Inria for larger domain shift (US city, very different)\n",
    "\n",
    "### Code Adaptation Checklist\n",
    "\n",
    "When adapting code from repositories above:\n",
    "\n",
    "- [ ] **Data loader**: Modify to load Geneva dataset format\n",
    "- [ ] **Number of classes**: Change from N-way to binary (rooftop/background)\n",
    "- [ ] **Image size**: Adapt to 250x250 (Geneva's size)\n",
    "- [ ] **Episodic sampling**: Sample from single grid (1301_11) instead of class-based sampling\n",
    "- [ ] **Evaluation**: Add geographic split evaluation (test on different grids)\n",
    "- [ ] **Metrics**: Ensure IoU computation is correct for binary segmentation\n",
    "\n",
    "### Quick Start Code Snippets\n",
    "\n",
    "#### Episodic Data Loader (adapt from PFENet)\n",
    "```python\n",
    "class EpisodicDataLoader:\n",
    "    def __init__(self, grid_data, n_shot=5, n_query=5):\n",
    "        self.grid_data = grid_data  # List of (image, mask) pairs\n",
    "        self.n_shot = n_shot\n",
    "        self.n_query = n_query\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            # Sample K support + Q query from grid\n",
    "            indices = np.random.choice(len(self.grid_data), \n",
    "                                      self.n_shot + self.n_query, \n",
    "                                      replace=False)\n",
    "            \n",
    "            support_idx = indices[:self.n_shot]\n",
    "            query_idx = indices[self.n_shot:]\n",
    "            \n",
    "            support_imgs = [self.grid_data[i][0] for i in support_idx]\n",
    "            support_masks = [self.grid_data[i][1] for i in support_idx]\n",
    "            query_imgs = [self.grid_data[i][0] for i in query_idx]\n",
    "            query_masks = [self.grid_data[i][1] for i in query_idx]\n",
    "            \n",
    "            yield (support_imgs, support_masks, query_imgs, query_masks)\n",
    "```\n",
    "\n",
    "#### Masked Average Pooling (adapt from PANet/HSNet)\n",
    "```python\n",
    "def masked_average_pooling(features, masks):\n",
    "    \\\"\\\"\\\"\n",
    "    Compute prototypes using masked average pooling.\n",
    "    \n",
    "    Args:\n",
    "        features: [K, C, H, W] - support features\n",
    "        masks: [K, H, W] - support masks\n",
    "    \n",
    "    Returns:\n",
    "        fg_proto, bg_proto: [C] - class prototypes\n",
    "    \\\"\\\"\\\"\n",
    "    # Resize masks to feature resolution\n",
    "    masks = F.interpolate(masks.unsqueeze(1).float(), \n",
    "                         size=features.shape[-2:], \n",
    "                         mode='nearest').squeeze(1)\n",
    "    \n",
    "    # Foreground prototype\n",
    "    fg_mask = (masks == 1).unsqueeze(1)  # [K, 1, H, W]\n",
    "    fg_features = features * fg_mask\n",
    "    fg_proto = fg_features.sum(dim=(0, 2, 3)) / (fg_mask.sum() + 1e-5)\n",
    "    \n",
    "    # Background prototype  \n",
    "    bg_mask = (masks == 0).unsqueeze(1)\n",
    "    bg_features = features * bg_mask\n",
    "    bg_proto = bg_features.sum(dim=(0, 2, 3)) / (bg_mask.sum() + 1e-5)\n",
    "    \n",
    "    return fg_proto, bg_proto\n",
    "```\n",
    "\n",
    "### Additional Resources for Debugging\n",
    "\n",
    "1. **Visualization Tools**\n",
    "   - Use `tensorboard` for training monitoring\n",
    "   - Use `matplotlib` for prototype visualization\n",
    "   - Use `sklearn.manifold.TSNE` for embedding visualization\n",
    "\n",
    "2. **Debugging Few-Shot Learning**\n",
    "   - Common issue: Prototypes collapse (all embeddings same)\n",
    "   - Solution: Check learning rate, add normalization\n",
    "   - Test on PASCAL-5i first to verify implementation works\n",
    "\n",
    "3. **Expected Results (Sanity Checks)**\n",
    "   - Zero-shot on Geneva grids: ~0.45-0.55 IoU\n",
    "   - Fine-tuning K=5: ~0.60-0.70 IoU\n",
    "   - Prototypical K=5: ~0.65-0.75 IoU\n",
    "   - PANet K=5: ~0.70-0.80 IoU\n",
    "\n",
    "### Citation\n",
    "\n",
    "If you use these approaches in your tutorial, cite the key papers:\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{wang2019panet,\n",
    "  title={PANet: Few-shot image semantic segmentation with prototype alignment},\n",
    "  author={Wang, Kaixin and Liew, Jun Hao and Zou, Yingtian and Zhou, Daquan and Feng, Jiashi},\n",
    "  booktitle={ICCV},\n",
    "  year={2019}\n",
    "}\n",
    "\n",
    "@inproceedings{snell2017prototypical,\n",
    "  title={Prototypical networks for few-shot learning},\n",
    "  author={Snell, Jake and Swersky, Kevin and Zemel, Richard},\n",
    "  booktitle={NeurIPS},\n",
    "  year={2017}\n",
    "}\n",
    "\n",
    "@misc{geneva-satellite-dataset,\n",
    "  title={Geneva Satellite Images Dataset},\n",
    "  author={OverfitTeam},\n",
    "  year={2024},\n",
    "  publisher={HuggingFace},\n",
    "  url={https://huggingface.co/datasets/raphaelattias/overfitteam-geneva-satellite-images}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. References & Resources\n",
    "\n",
    "### Academic Papers\n",
    "\n",
    "#### Few-Shot Learning Foundations\n",
    "\n",
    "1. **Prototypical Networks for Few-shot Learning**\n",
    "   - Snell, J., Swersky, K., & Zemel, R. (2017)\n",
    "   - NeurIPS 2017\n",
    "   - Paper: https://arxiv.org/abs/1703.05175\n",
    "   - *Foundation paper for prototypical networks in classification*\n",
    "\n",
    "2. **Model-Agnostic Meta-Learning (MAML)**\n",
    "   - Finn, C., Abbeel, P., & Levine, S. (2017)\n",
    "   - ICML 2017\n",
    "   - Paper: https://arxiv.org/abs/1703.03400\n",
    "   - *Alternative meta-learning approach (more complex)*\n",
    "\n",
    "#### Few-Shot Semantic Segmentation\n",
    "\n",
    "3. **PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment**\n",
    "   - Wang, K., Liew, J. H., Zou, Y., Zhou, D., & Feng, J. (2019)\n",
    "   - ICCV 2019\n",
    "   - Paper: https://arxiv.org/abs/1908.06391\n",
    "   - GitHub: https://github.com/kaixin96/PANet\n",
    "   - *The main PANet paper - state-of-art few-shot segmentation*\n",
    "\n",
    "4. **Adaptive Masked Proxies for Few-Shot Segmentation**\n",
    "   - Boudiaf, M., Kervadec, H., Masud, Z. I., Piantanida, P., Ben Ayed, I., & Dolz, J. (2021)\n",
    "   - ICCV 2021\n",
    "   - Paper: https://arxiv.org/abs/2102.11123\n",
    "   - GitHub: https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation\n",
    "   - *Improved version of PANet with adaptive prototypes*\n",
    "\n",
    "5. **HSNet: Hypercorrelation Squeeze for Few-Shot Segmentation**\n",
    "   - Min, J., Kang, D., & Cho, M. (2021)\n",
    "   - ICCV 2021\n",
    "   - Paper: https://arxiv.org/abs/2109.06211\n",
    "   - GitHub: https://github.com/juhongm999/hsnet\n",
    "   - *Alternative approach using correlation matching*\n",
    "\n",
    "6. **Meta-Learning for Few-Shot Semantic Segmentation**\n",
    "   - Tian, P., Wu, Z., Qi, L., Wang, L., Shi, Y., & Gao, Y. (2020)\n",
    "   - Pattern Recognition 2020\n",
    "   - Paper: https://arxiv.org/abs/2004.07730\n",
    "   - *Good survey of meta-learning for segmentation*\n",
    "\n",
    "#### Remote Sensing & Building Segmentation\n",
    "\n",
    "7. **Building Extraction from Remote Sensing Images with Deep Learning**\n",
    "   - Ji, S., Wei, S., & Lu, M. (2019)\n",
    "   - Remote Sensing 2019\n",
    "   - Paper: https://www.mdpi.com/2072-4292/11/7/778\n",
    "   - *Survey of building segmentation methods*\n",
    "\n",
    "8. **Domain Adaptation for Semantic Segmentation of Urban Scenes**\n",
    "   - Hoffman, J., et al. (2018)\n",
    "   - CVPR 2018\n",
    "   - Paper: https://arxiv.org/abs/1711.06969\n",
    "   - *Related work on domain adaptation for urban scenes*\n",
    "\n",
    "### GitHub Repositories\n",
    "\n",
    "#### Few-Shot Segmentation Implementations\n",
    "\n",
    "1. **PANet (Official)**\n",
    "   - https://github.com/kaixin96/PANet\n",
    "   - TensorFlow implementation of PANet\n",
    "   - Includes PASCAL-5i and COCO-20i benchmarks\n",
    "\n",
    "2. **Few-Shot Semantic Segmentation Papers and Code**\n",
    "   - https://github.com/xiaomengyc/Few-Shot-Semantic-Segmentation-Papers\n",
    "   - Comprehensive collection of papers and code\n",
    "   - Updated regularly with new methods\n",
    "\n",
    "3. **PFENet: Prior Guided Feature Enrichment Network**\n",
    "   - https://github.com/dvlab-research/PFENet\n",
    "   - TPAMI 2020\n",
    "   - PyTorch, very clean implementation\n",
    "   - Good starting point for understanding few-shot segmentation\n",
    "\n",
    "4. **HSNet (Hypercorrelation Squeeze Network)**\n",
    "   - https://github.com/juhongm999/hsnet\n",
    "   - PyTorch implementation\n",
    "   - State-of-art results, well-documented\n",
    "\n",
    "5. **RePRI: Adaptive Masked Proxies**\n",
    "   - https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation\n",
    "   - PyTorch implementation\n",
    "   - Improved PANet variant\n",
    "\n",
    "#### Prototypical Networks Implementations\n",
    "\n",
    "6. **Prototypical Networks (PyTorch)**\n",
    "   - https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch\n",
    "   - Clean PyTorch reimplementation\n",
    "   - Good for understanding prototypical networks\n",
    "\n",
    "7. **Few-Shot Learning Library (learn2learn)**\n",
    "   - https://github.com/learnables/learn2learn\n",
    "   - PyTorch library for meta-learning\n",
    "   - Includes MAML, Prototypical Networks, and more\n",
    "   - Great for experimentation\n",
    "\n",
    "#### Building/Rooftop Segmentation\n",
    "\n",
    "8. **SpaceNet Building Detection**\n",
    "   - https://github.com/SpaceNetChallenge/BuildingDetectors\n",
    "   - Collection of winning solutions for building detection\n",
    "   - Various architectures (U-Net, Mask R-CNN, etc.)\n",
    "\n",
    "9. **Inria Aerial Image Labeling Benchmark**\n",
    "   - https://github.com/zorzi-s/projectRegularization\n",
    "   - Segmentation on Inria dataset\n",
    "   - Building footprint extraction\n",
    "\n",
    "10. **Rooftop Segmentation for Solar Potential**\n",
    "    - https://github.com/mdominguezd/Solar-Panel-Installability\n",
    "    - Solar panel rooftop segmentation\n",
    "    - Similar application to our use case\n",
    "\n",
    "### Datasets\n",
    "\n",
    "#### Few-Shot Segmentation Benchmarks\n",
    "\n",
    "1. **PASCAL-5i**\n",
    "   - Standard few-shot segmentation benchmark\n",
    "   - 4-fold cross-validation setup\n",
    "   - Download: http://host.robots.ox.ac.uk/pascal/VOC/\n",
    "\n",
    "2. **COCO-20i**\n",
    "   - MS COCO adapted for few-shot segmentation\n",
    "   - 20 classes, 4 splits\n",
    "   - Download: https://cocodataset.org/\n",
    "\n",
    "#### Remote Sensing & Building Datasets\n",
    "\n",
    "3. **Inria Aerial Image Dataset**\n",
    "   - 5 cities, 180 km² coverage\n",
    "   - Building footprints\n",
    "   - Download: https://project.inria.fr/aerialimagelabeling/\n",
    "   - HuggingFace: https://huggingface.co/datasets/Jonathan/INRIA-Aerial-Dataset\n",
    "\n",
    "4. **SpaceNet Building Dataset**\n",
    "   - Multiple cities worldwide\n",
    "   - High-resolution satellite imagery\n",
    "   - Download: https://spacenet.ai/datasets/\n",
    "\n",
    "5. **Massachusetts Buildings Dataset**\n",
    "   - 151 aerial images\n",
    "   - Building segmentation masks\n",
    "   - Download: https://www.cs.toronto.edu/~vmnih/data/\n",
    "\n",
    "6. **Open Cities AI Challenge Dataset**\n",
    "   - African cities building footprints\n",
    "   - Good for domain adaptation\n",
    "   - Download: https://www.drivendata.org/competitions/60/building-segmentation-disaster-resilience/\n",
    "\n",
    "7. **xBD (xView2) Building Damage Dataset**\n",
    "   - Pre/post disaster building assessment\n",
    "   - Multiple cities\n",
    "   - Download: https://xview2.org/\n",
    "\n",
    "### Related Tutorials & Blog Posts\n",
    "\n",
    "1. **Few-Shot Learning Tutorial**\n",
    "   - https://lilianweng.github.io/posts/2018-11-30-meta-learning/\n",
    "   - Excellent overview of meta-learning approaches\n",
    "   - By Lilian Weng (OpenAI)\n",
    "\n",
    "2. **Prototypical Networks Explained**\n",
    "   - https://towardsdatascience.com/prototypical-networks-for-few-shot-learning-eb2c2b86baac\n",
    "   - Intuitive explanation with code examples\n",
    "\n",
    "3. **Satellite Image Segmentation**\n",
    "   - https://github.com/robmarkcole/satellite-image-deep-learning\n",
    "   - Comprehensive resource for satellite image analysis\n",
    "   - Includes segmentation techniques\n",
    "\n",
    "### Useful Libraries\n",
    "\n",
    "1. **segmentation_models.pytorch**\n",
    "   - https://github.com/qubvel/segmentation_models.pytorch\n",
    "   - Pre-built segmentation architectures (U-Net, DeepLab, etc.)\n",
    "   - Good for baseline implementations\n",
    "\n",
    "2. **Albumentations**\n",
    "   - https://github.com/albumentations-team/albumentations\n",
    "   - Data augmentation library\n",
    "   - Essential for episodic training\n",
    "\n",
    "3. **PyTorch Metric Learning**\n",
    "   - https://github.com/KevinMusgrave/pytorch-metric-learning\n",
    "   - Useful for prototypical networks\n",
    "   - Distance metrics, losses, miners\n",
    "\n",
    "### Community & Forums\n",
    "\n",
    "1. **Papers with Code - Few-Shot Segmentation**\n",
    "   - https://paperswithcode.com/task/few-shot-semantic-segmentation\n",
    "   - Leaderboards, benchmarks, and implementations\n",
    "\n",
    "2. **r/MachineLearning - Meta-Learning discussions**\n",
    "   - https://www.reddit.com/r/MachineLearning/\n",
    "   - Active community for questions\n",
    "\n",
    "3. **Computer Vision Foundation (CVF) Open Access**\n",
    "   - https://openaccess.thecvf.com/\n",
    "   - Free access to CVPR/ICCV/ECCV papers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
